{"data":{"allMdx":{"totalCount":2,"edges":[{"node":{"id":"9b9ee6b4-9ba8-5b85-9e3f-4aabea530990","body":"\r\nimport { Box } from \"@mui/system\";\r\nimport {Divider, Typography} from \"@mui/material\";\r\n\r\n<Box sx={{display: 'flex', flexDirection: 'row', justifyContent: \"space-between\", alignItems: \"end\", mb: 0}}>\r\n    <Typography variant=\"h1\" sx={{pt: 5}} gutterBottom>\r\n        # {props.pageContext.frontmatter.title}\r\n    </Typography>\r\n    <Typography variant=\"h5\" color=\"textSecondary\" gutterBottom>\r\n        {props.pageContext.frontmatter.date}\r\n    </Typography>\r\n</Box>\r\n<Divider sx={{\r\n    background: 'linear-gradient(to right, #ff4081, #1b1b1b)',\r\n    height: '2px',\r\n    width: \"100%\",\r\n    borderRadius: '4px',\r\n    margin: '2px 0',\r\n    boxShadow: '0 0 10px rgba(255, 64, 129, 0.1)' // Adds a glowing shadow effect\r\n}}/>\r\n\r\n<Box>\r\n\r\n</Box>\r\n&nbsp;\r\n## Ten Minutes of OpenSSL\r\n\r\nOpenSSL is a widely used open-source cryptographic toolkit and SSL/TLS implementation.\r\n\r\nIt provides:\r\n\r\n- A cryptographic library used by applications\r\n- A CLI tool called `openssl` for working with:\r\n  - Certificates\r\n  - Keys\r\n  - TLS connections\r\n  - Hashes\r\n  - <Term name=\"SAN\" prefix=\"TLS\"/>\r\n  - Encryption and decryption\r\n  - CSRs and CAs\r\n\r\n\r\n\r\nIf you have ever generated a TLS certificate, built a private CA, debugged a TLS handshake, created a CSR, or inspected an X.509 certificate ‚Äî you were using OpenSSL directly or indirectly.\r\n\r\nThis article builds a layered mental model so you can hold the TLS and PKI stack together in memory.\r\n\r\n<Divider />\r\n\r\n## Core mental model ‚Äî what TLS actually does\r\n\r\nTLS performs two separate cryptographic jobs:\r\n\r\n```text\r\nIdentity proof  = certificates and signatures\r\nTraffic secrecy = symmetric session encryption\r\n```\r\n\r\nCertificates prove identity. Session keys encrypt traffic.\r\n\r\nOpenSSL gives you the tools to build, inspect, verify, and debug both layers.\r\n\r\n---\r\n\r\n## Layer 1 ‚Äî Keys: root of identity\r\n\r\nEverything starts with a key pair:\r\n\r\n```text\r\nprivate key = secret\r\npublic key  = shareable\r\n```\r\n\r\nGenerate a private key:\r\n\r\n```bash\r\nopenssl genrsa -out server.key.pem 2048\r\n```\r\n\r\nPrivate keys are used to:\r\n\r\n- prove identity\r\n- sign handshakes\r\n- sign CSRs\r\n- sign certificates (if acting as a CA)\r\n\r\nIf the private key leaks, identity is compromised.\r\n\r\n---\r\n\r\n## Layer 2 ‚Äî X.509 certificates\r\n\r\nAn X.509 certificate binds identity to a public key and a CA signature:\r\n\r\n```text\r\nidentity + public key + CA signature = certificate\r\n```\r\n\r\nCertificates contain:\r\n\r\n- Subject identity fields\r\n- Public key\r\n- Validity window\r\n- Extensions (SAN, key usage, EKU)\r\n- Issuer\r\n- Signature\r\n\r\nInspect a certificate:\r\n\r\n```bash\r\nopenssl x509 -in cert.pem -text -noout\r\n```\r\n\r\nX.509 defines structure, not encoding.\r\n\r\n---\r\n\r\n## Layer 3 ‚Äî DN fields vs SAN\r\n\r\nDistinguished Name (DN) fields come from directory systems (X.500 / X.509 heritage):\r\n\r\n```text\r\nCN = Common Name\r\nO  = Organization\r\nOU = Organizational Unit\r\nC  = Country\r\nST = State\r\nL  = Locality\r\n```\r\n\r\nThese fields are mostly descriptive.\r\n\r\nOperational identity lives in SAN (Subject Alternative Name). Example SAN values:\r\n\r\n```text\r\nDNS:api.example.com\r\nIP:10.0.0.5\r\nURI:spiffe://prod/payment\r\n```\r\n\r\nModern TLS hostname validation checks SAN, not CN.\r\n\r\nPractical check:\r\n\r\n```bash\r\nopenssl x509 -in cert.pem -noout -ext subjectAltName\r\n```\r\n\r\n---\r\n\r\n## Layer 4 ‚Äî CSR: certificate signing requests\r\n\r\nA CSR is a certificate application form (standard: PKCS#10). It contains:\r\n\r\n- your public key\r\n- requested identity fields\r\n- requested SAN entries\r\n- your signature proving key ownership\r\n\r\nCreate a CSR:\r\n\r\n```bash\r\nopenssl req -new -key server.key.pem -out server.csr.pem\r\n```\r\n\r\nFlow:\r\n\r\n```text\r\nkey -> CSR -> CA signs -> certificate\r\n```\r\n\r\n---\r\n\r\n## Layer 5 ‚Äî Certificate Authorities (CA) and trust chains\r\n\r\nA Certificate Authority is simply a key that signs certificates.\r\n\r\nCreate a lab Root CA:\r\n\r\n```bash\r\nopenssl genrsa -out ca.key.pem 4096\r\n\r\nopenssl req -x509 -new -key ca.key.pem   -out ca.crt.pem -days 3650   -subj \"/C=DK/O=Lab PKI/CN=Lab Root CA\"\r\n```\r\n\r\nClients trust CA certificates via a trust store. Verification builds a chain:\r\n\r\n```text\r\nleaf (server) -> intermediate(s) -> root (trusted)\r\n```\r\n\r\nVerify a server cert against a CA file:\r\n\r\n```bash\r\nopenssl verify -CAfile ca.crt.pem server.crt.pem\r\n```\r\n\r\n---\r\n\r\n## Layer 6 ‚Äî File formats and containers (PEM, DER, PFX)\r\n\r\nX.509 is the data structure. PEM and DER are encodings.\r\n\r\nPEM is text (Base64 with headers), for example:\r\n\r\n```text\r\n-----BEGIN CERTIFICATE-----\r\n(base64)\r\n-----END CERTIFICATE-----\r\n```\r\n\r\nDER is binary ASN.1 encoding.\r\n\r\nPFX (PKCS#12) is an encrypted bundle containing:\r\n\r\n- private key\r\n- certificate\r\n- chain (optional)\r\n\r\nConvert PEM to PFX:\r\n\r\n```bash\r\nopenssl pkcs12 -export   -inkey server.key.pem   -in server.crt.pem   -certfile ca.crt.pem   -out server.pfx\r\n```\r\n\r\nInspect a PFX:\r\n\r\n```bash\r\nopenssl pkcs12 -in server.pfx -info -noout\r\n```\r\n\r\n---\r\n\r\n## Layer 7 ‚Äî TLS handshake: how encryption actually starts\r\n\r\nSimplified TLS handshake:\r\n\r\n```text\r\nclient hello\r\nserver sends certificate (and chain)\r\nclient verifies chain\r\nclient verifies SAN hostname match\r\nkey agreement (usually ECDHE)\r\nsession keys derived\r\nencrypted channel begins\r\n```\r\n\r\nModern TLS typically uses:\r\n\r\n- ECDHE for key agreement (forward secrecy)\r\n- AES-GCM or ChaCha20-Poly1305 for traffic encryption (symmetric)\r\n\r\nThe server private key is used to sign handshake data to prove identity (possession of the private key). Traffic is not encrypted with the server private key.\r\n\r\n---\r\n\r\n## Layer 8 ‚Äî How certificates are asserted true\r\n\r\nClient validation checks commonly include:\r\n\r\n- CA signature chain validation\r\n- Validity window (Not Before / Not After)\r\n- SAN hostname match\r\n- Extended Key Usage (serverAuth / clientAuth)\r\n- Revocation status (CRL / OCSP, where used)\r\n- Policy constraints (in strict environments)\r\n\r\nThree proofs to remember:\r\n\r\n```text\r\nCA signature  -> issuer authenticity\r\nprivate key   -> possession proof (live handshake signing)\r\nSAN match     -> name binding to the endpoint you contacted\r\n```\r\n\r\n---\r\n\r\n## Layer 9 ‚Äî TLS debugging with OpenSSL\r\n\r\nMost useful diagnostic command:\r\n\r\n```bash\r\nopenssl s_client -connect host:port -servername hostname\r\n```\r\n\r\nIt shows:\r\n\r\n- certificate chain presented by server\r\n- negotiated TLS version and cipher\r\n- verification results and errors\r\n\r\nForce hostname verification (useful for catching SAN mistakes):\r\n\r\n```bash\r\nopenssl s_client -connect host:port -servername hostname   -verify_hostname hostname -verify_return_error\r\n```\r\n\r\n---\r\n\r\n## Layer 10 ‚Äî Service identity: SPIFFE and SPIRE\r\n\r\nIn zero-trust and service-mesh environments, identities are often expressed as URIs in SAN:\r\n\r\n```text\r\nURI:spiffe://domain/workload\r\n```\r\n\r\n- SPIFFE defines the identity format (SPIFFE ID).\r\n- SPIRE automates issuance and rotation of short-lived X.509 certificates containing that URI SAN.\r\n\r\nThis is still X.509 and TLS; it is just modernized identity and automation on top.\r\n\r\n---\r\n\r\n# Appendix ‚Äî terminology, abbreviations, and acronyms\r\n\r\n<PageGlossary />\r\n\r\n#","frontmatter":{"slug":"blog/ten-minutes-of-openssl","date":"2026-02-07","title":"OpenSSL Demystified","description":"A practical, systems-level deep dive into OpenSSL, X.509 certificates, TLS handshakes, PKI files, and trust chains ‚Äî with real commands and mental models.","categories":["Security","Systems Programming"],"tags":["OpenSSL","TLS","PKI","X509","Certificates","Security"],"featured":true,"series":"Understanding Systems Deeply"}}},{"node":{"id":"dec1324d-9d4c-55ef-8799-c30e9de63a5f","body":"\r\nimport StructuredNote from \"../../components/mdxblog/StructuredNote\";\r\nimport EncodingBoundary from \"../../components/encoding-boundary/EncodingBoundary\";\r\nimport {Container, Divider, Link, Paper, Typography} from \"@mui/material\";\r\nimport {Box} from \"@mui/system\";\r\nimport ZoomableImage from \"../../components/mdxblog/ZoomableImage\";\r\nimport ImageWithText from \"../../components/mdxblog/ImageParagraph\";\r\nimport BlogLink from \"../../components/mdxblog/BlogLink\";\r\nimport ShadowComment from \"../../components/mdxblog/ShadowComment\";\r\nimport HelperLinkText from \"../../components/mdxblog/HelperLinkText\";\r\n\r\n<ArticleHeader\r\n    title={props.pageContext.frontmatter.title}\r\n    subtitle={props.pageContext.frontmatter.subtitle}\r\n    date={props.pageContext.frontmatter.date}\r\n/>\r\n\r\nThe <BlogLink href=\"https://www.unicode.org/standard/standard.html\">Unicode Standard</BlogLink> is a universal\r\nframework for encoding, representing, and managing text across digital systems ‚Äî including computers, databases, and\r\noperating systems. Introduced in 1991, and maintained by the <BlogLink\r\nhref=\"https://www.unicode.org/consortium/consort.html\">Unicode Consortium</BlogLink>, it defines more than **150,000\r\ncharacters** ‚Äî covering everything from modern scripts to ancient, even extinct languages.\r\nFor example, thanks to Unicode, we can now easily render languages like <BlogLink\r\nhref=\"https://www.cbc.ca/news/canada/thunder-bay/%E1%90%8A%E1%90%A3%E1%91%95%E1%94%9A%E1%90%A3%E1%90%A6%E1%90%83-%E1%90%81%E1%90%8F%E1%92%8B%E1%91%AD%E1%90%8D-%E1%90%85%E1%91%95%E1%90%B1%E1%93%87%E1%92%AA%E1%90%A3-%E1%93%82%E1%92%AA%E1%92%AA-%E1%90%85%E1%91%8E%E1%94%91%E1%91%AD%E1%94%91%E1%90%8D%E1%90%8F%E1%90%A3-%E1%92%B7%E1%94%A6-%E1%90%83%E1%90%A1%E1%91%BF%E1%93%AD%E1%90%A0-1.6385526\"\r\nhoverText=\"Oji-Cree article on CBC\">Oji-Cree</BlogLink> ‚Äî a historically endangered language ‚Äî directly in a\r\nbrowser. <HelperLinkText\r\nhref=\"https://www.cbc.ca/news/canada/thunder-bay/first-person-rochelle-bragg-reclaiming-language-1.6371587\"\r\npopoverText=\"Translation article\">(translated here)</HelperLinkText>.\r\n<ShadowComment>It‚Äôs pretty mind-blowing that prehistoric scripts can be rendered effortlessly online,\r\n    right?</ShadowComment>\r\n\r\n<Typography variant=\"h3\" fontStyle=\"bold\">How This Deep Dive Will Work</Typography>\r\n\r\n- Each section tackles one key concept or layer of Unicode.\r\n- **Summary tables** at the end of each section help reinforce learning.\r\n- **Real Python code examples** are provided throughout ‚Äî so you can immediately apply the concepts hands-on.\r\n\r\n\r\n<StructuredNote type=\"info\">\r\n    When a ‚ß´ appear next to a term, you can hover or click that word and have it lookup a structured explanation of what\r\n    it means, applies to, examples and more.\r\n</StructuredNote>\r\n\r\nBy the end, you‚Äôll have a **practical, layered understanding** of Unicode ‚Äî from its historical roots to its modern applications in encoding, text processing, and even security.\r\n\r\nBecause of its **ambition to be the final text encoding standard**, Unicode is a **layered** and **multi-faceted system**. Working with Unicode can seem complex until you grasp those different layers, facets and details; this article aims to explore this carefully by throughful analyses, and by the end of it, my goal for you is that you have a clear mental model of Unicode ‚Äî what it replaced and how it functions.\r\n\r\n<Divider sx={{my: 5, mx: 2}}></Divider>\r\n\r\n## Layer Zero: Pre-Unicode ‚Äî ASCII and Code Pages\r\n### The Era Before Unicode\r\n\r\n\r\n<ImageWithText\r\n    imageSrc=\"/graphics/blog/10-minutes-of-unicode/mojibake01.png\"\r\n    altText=\"Example of mojibake caused by encoding mismatch\"\r\n    caption=\"Mojibake: text decoded using the wrong character encoding.\"\r\n    imageWidth=\"200px\"\r\n>\r\n    Before Unicode, text encoding was fragmented into various character encodings specific to\r\n    languages or regions (e.g., ASCII, ISO 8859, and Windows-1252). These encoding standards often conflicted,\r\n    resulting in garbled text when transferred across systems that used different encoding schemes - a phenomenon that\r\n    has its own name: *mojibake* (Japanese for \"character transformation\", or simply: gibberish).\r\n</ImageWithText>\r\nThe main initial takeaway is this: Unicode **standardizes** the *representation* of text across different languages\r\nand platforms. Encodings (like <Term name=\"UTF8\"/>) *implement* Unicode by translating its <Term name=\"CodePoint\"\r\n                                                                                                 ending=\"s\"/> into\r\nbytes. Each character we see is simply not just a letter or a symbol - that's its representation (aka. <Term\r\n    name=\"Glyph\"/>) - its true nature within the computer is a defined code point, encoded into bytes and then later\r\nrendered as a glyph.\r\n\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer One: Why Encoding Matters\r\n### Why There‚Äôs No Such Thing as \"Plain Text\"\r\n\r\nWhen dealing with text as bytes, knowing the encoding used to generate those bytes is crucial.\r\nThis is especially true in I/O operations just take a look at the following examples:\r\n\r\nEncoding is often specified by context, for example:\r\n- Over the internet, it‚Äôs specified in HTTP headers (e.g., `\"Content-Type: text/html; charset=UTF-8\"`).\r\n- In HTML or XML files, it appears in metadata (`<meta charset=\"UTF-8\">`).\r\n- In databases (`ENCODING 'UTF8'`)\r\n\r\nBut there places that even bites experienced engineers - an example of such are log files which may be whatever encoding the OS feels like, or it may be in an implicit format and working fine until it suddenly breaks; corrupting a database, taking a system offline or something far worse.\r\n\r\nEncoding may also be an emergent implicit behaviour of OS-defaults, locale configuration, runtime assumptions, libraries silently coverting strings to bytes or tooling that tries to be helpful - and as long as all components accidentially agrees, everything works ‚Äî the moment one component disagress however, is where the damage happends (and that's how real production failures happen).\r\n\r\nLogfiles are often treated as diagnostic exhaust, not something you take much care of which means encoding is rarely specified, validated or tested end-to-end.\r\nAnd they sit at the intersection of:\r\n* Application code\r\n* OS\r\n* Runtime\r\n* Pipelines\r\n* Storage backend\r\n* Viewer/UI\r\n\r\nAnd nobody owns encoding end-to-end.\r\n\r\n<EncodingBoundary\r\n    initialScenarioId=\"mysql-utf8mb3\"\r\n    scenarios={[\r\n        {\r\n            id: \"mysql-utf8mb3\",\r\n            label: \"MySQL 'utf8' silently drops 4-byte characters\",\r\n            source: \"Web application\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"User status: üí©\",\r\n            bytes: \"55 73 65 72 20 73 74 61 74 75 73 3A 20 F0 9F 92 A9\",\r\n            target: \"MySQL database\",\r\n            targetEncoding: \"utf8 (mb3)\",\r\n            targetText: \"User status: ÔøΩ\",\r\n            description:\r\n                \"MySQL‚Äôs legacy 'utf8' encoding only supports up to 3 bytes per character. Emoji are silently truncated or replaced, corrupting production data without errors.\",\r\n        },\r\n        {\r\n            id: \"log-pipeline-mojibake\",\r\n            label: \"UTF-8 logs misread by ingestion pipeline\",\r\n            source: \"Backend service\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"latency=123¬µs\",\r\n            bytes: \"6C 61 74 65 6E 63 79 3D 31 32 33 C2 B5 73\",\r\n            target: \"Log aggregator\",\r\n            targetEncoding: \"ISO-8859-1\",\r\n            targetText: \"latency=123√Ç¬µs\",\r\n            description:\r\n                \"Logs are emitted as UTF-8 but decoded as Latin-1 downstream. Systems remain operational, but metrics and forensic timelines become unreliable.\",\r\n        },\r\n        {\r\n            id: \"csv-excel-corruption\",\r\n            label: \"UTF-8 CSV opened as Windows-1252\",\r\n            source: \"Reporting service\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"Jos√© Garc√≠a\",\r\n            bytes: \"4A 6F 73 C3 A9 20 47 61 72 63 C3 AD 61\",\r\n            target: \"Excel\",\r\n            targetEncoding: \"Windows-1252\",\r\n            targetText: \"Jos√É¬© Garc√É¬≠a\",\r\n            description:\r\n                \"CSV files exported as UTF-8 are opened in Excel without explicit encoding. Names are corrupted in payroll, finance, and legal documents.\",\r\n        },\r\n        {\r\n            id: \"unicode-normalization-auth\",\r\n            label: \"Authentication fails due to Unicode normalization\",\r\n            source: \"User input\",\r\n            sourceNormalizationFormat: null,\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"caf√©\",\r\n            bytes: \"63 61 66 65 CC 81\",\r\n            target: \"Auth service\",\r\n            targetNormalizationFormat: \"NFC\",\r\n            targetEncoding: \"Unicode (NFC)\",\r\n            targetText: \"caf√©\",\r\n            description:\r\n                \"Visually identical strings with different Unicode normalization forms fail equality checks, breaking authentication in real-world systems.\",\r\n        },\r\n        {\r\n            id: \"ascii-api-gateway\",\r\n            label: \"API gateway assumes ASCII\",\r\n            source: \"Client SDK\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"Êù±‰∫¨\",\r\n            bytes: \"E6 9D B1 E4 BA AC\",\r\n            target: \"API gateway\",\r\n            targetEncoding: \"ASCII\",\r\n            targetText: \"??\",\r\n            description:\r\n                \"An API gateway validates requests as ASCII-only. International customer data is rejected or corrupted after launch.\",\r\n        },\r\n        {\r\n            id: \"jwt-signature-encoding\",\r\n            label: \"JWT signature verification fails due to encoding mismatch\",\r\n            source: \"Auth issuer\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"header.\\<ZWSP\\>payload\",\r\n            bytes: \"2E E2 80 8B E2 72 2E\",\r\n            target: \"API verifier\",\r\n            targetEncoding: \"ASCII\",\r\n            targetText: \"header.\",\r\n            description:\r\n                \"A zero-width Unicode character (ZWSP) survives signing but is stripped during verification. Tokens randomly fail validation, causing intermittent authentication outages.\",\r\n        },\r\n        {\r\n            id: \"filesystem-db-encoding\",\r\n            label: \"Filename stored differently than filesystem path\",\r\n            source: \"Upload service\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"r√©sum√©.pdf\",\r\n            bytes: \"72 C3 A9 73 75 6D C3 A9 2E 70 64 66\",\r\n            target: \"Database\",\r\n            targetEncoding: \"Unicode (NFD)\",\r\n            targetText: \"resumeÃÅ.pdf\",\r\n            description:\r\n                \"The database and filesystem normalize Unicode differently. Files exist on disk but cannot be retrieved, deleted, or audited.\",\r\n        },\r\n        {\r\n            id: \"queue-poison-pill\",\r\n            label: \"Message queue consumer crashes on unexpected encoding\",\r\n            source: \"Producer service\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"event=‚úì\",\r\n            bytes: \"65 76 65 6E 74 3D E2 9C 93\",\r\n            target: \"Consumer service\",\r\n            targetEncoding: \"ASCII\",\r\n            targetText: \"\",\r\n            description:\r\n                \"A single UTF-8 message crashes an ASCII-only consumer. The message is retried indefinitely, blocking the entire partition.\",\r\n        },\r\n        {\r\n            id: \"utf8-bom-header-break\",\r\n            label: \"UTF-8 BOM breaks CSV / JSON header parsing\",\r\n            source: \"Reporting service\",\r\n            sourceEncoding: \"UTF-8 BOM\",\r\n            sourceText: \"id,name,email\",\r\n            bytes: \"EF BB BF 69 64 2C 6E 61 6D 65 2C 65 6D 61 69 6C\",\r\n            target: \"Data ingestion pipeline\",\r\n            targetEncoding: \"UTF-8\",\r\n            targetText: \"id,name,email\",\r\n            description:\r\n                \"A UTF-8 Byte Order Mark (BOM) is preserved at the start of the payload. Downstream systems treat the first header or JSON key as a different string, causing missing fields, broken mappings, or silent data loss.\"\r\n        },\r\n        {\r\n            id: \"mixed-unicode-normalization\",\r\n            label: \"Decomposed Unicode normalization\",\r\n            source: \"User input pipeline\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"caf√©\",\r\n            bytes: \"63 61 66 C3 A9 CC 81\",\r\n            target: \"Application logic\",\r\n            targetEncoding: \"Unicode (mixed NFC/NFD)\",\r\n            targetText: \"caf\\u00E9\\u0301\",\r\n            description:\r\n                \"A string contains a mix of composed and decomposed Unicode characters. Partial normalization causes visually identical strings to fail equality checks, cache lookups, and hash comparisons.\"\r\n        },\r\n        {\r\n            id: \"json-bytes-vs-text-signature\",\r\n            label: \"JSON signature breaks due to string vs bytes ambiguity\",\r\n            source: \"Client\",\r\n            sourceEncoding: \"UTF-8\",\r\n            sourceText: \"{\\\"amount\\\":\\\"100\\\",\\\"note\\\":\\\"paid\\\"}\",\r\n            bytes: \"7B 22 61 6D 6F 75 6E 74 22 3A 22 31 30 30 22 2C 22 6E 6F 74 65 22 3A 22 70 61 69 64 22 7D\",\r\n            target: \"API gateway\",\r\n            targetEncoding: \"UTF-8\",\r\n            targetText: \"b'{\\\"amount\\\":\\\"100\\\",\\\"note\\\":\\\"paid\\\"}'\",\r\n            description:\r\n                \"The payload is treated as a decoded text in one system and as bytes (b'...') in another, causing signatures to be computed over different representations.\"\r\n        },\r\n    ]}\r\n/>\r\n\r\n#### Testing Encodings with Python\r\n\r\n```python{numberLines: true}\r\nimport sys\r\n\r\nif __name__ == '__main__':\r\n    # Python source files are assumed to be encoded in UTF-8 unless specified otherwise.\r\n    # You can change this behaviour by adding a string like this as the first line:\r\n    # -*- coding: <encoding-name> -*-\r\n\r\n    test_string = \"Hello World! üåç\" # But strings will be stored internally as Unicode codepoints in Python!\r\n    # This allows for seperation of data and encoding logic - adding flexibility.\r\n\r\n    # When priting to the console, it's the console that sets the encoding scheme\r\n    print(sys.stdout.encoding + \" \" + sys.stdin.encoding)  # utf-8 utf-8\r\n    print(test_string)  # Hello World! üåç\r\n\r\n\r\n    utf8_encoded: bytes = test_string.encode(\"utf-8\")\r\n    utf16_encoded: bytes = test_string.encode(\"utf-16\")\r\n    utf32_encoded: bytes = test_string.encode(\"utf-32\")\r\n\r\n    print(utf8_encoded)  # b'Hello World! \\xf0\\x9f\\x8c\\x8d'\r\n    print(utf16_encoded)  # b'\\xff\\xfeH\\x00e\\x00l\\x00l\\x00o\\x00 \\x00W\\x00o\\x00r\\x00l\\x00d\\x00!\\x00 \\x00<\\xd8\\r\\xdf'\r\n    print(utf32_encoded)  # b'\\xff\\xfe\\x00\\x00H\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00 \\x00\\x00\\x00W\\x00\\x00\\x00o\\x00\\x00\\x00r\\x00\\x00\\x00l\\x00\\x00\\x00d\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00\\r\\xf3\\x01\\x00'\r\n    # We can see here how the same string is represented differently at the byte level when using different encodings.\r\n\r\n    utf8_decoded: str = utf8_encoded.decode(\"utf-8\")\r\n    utf16_decoded: str = utf16_encoded.decode(\"utf-16\")\r\n    utf32_decoded: str = utf32_encoded.decode(\"utf-32\")\r\n    print(utf8_decoded)  # Hello World! üåç\r\n    print(utf16_decoded)  # Hello World! üåç\r\n    print(utf32_decoded)  # Hello World! üåç\r\n\r\n    \"\"\"\r\n    ascii_encoded: bytes = test_string.encode(\"ascii\")\r\n    latin1_encoded: bytes = test_string.encode(\"latin-1\")\r\n    windows1252_encoded: bytes = test_string.encode(\"windows-1252\")\r\n    \"\"\"\r\n    # These would all fail with a UnicodeEncodeError as the codepoints for the emoji doesn't exists and can't be encoded!\r\n    # You have to watch out for such cases when encoding and decoding data.\r\n    try:\r\n        ascii_encoded = test_string.encode(\"ascii\")\r\n    except UnicodeEncodeError:\r\n        print(\"Error: The string contains characters that cannot be encoded as ASCII.\")\r\n        ascii_encoded = test_string.encode(\"ascii\", errors=\"ignore\")\r\n    finally:\r\n        print(ascii_encoded) # b'Hello World! '\r\n```\r\n\r\nWhen encoding isn‚Äôt specified, heuristics are sometimes used as a legacy compatability-hack and was used in browsers like IE - but they are unreliable and should be avoided. Always specify encoding to avoid misinterpretation.\r\n\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer Two: Scripts, Symbols, and Categories\r\nThe first layer of Unicode is that of **scripts** and **symbols**.\r\n\r\n### Scripts\r\n<BlogLink href=\"https://en.wikipedia.org/wiki/Latin_alphabet\">Latin\r\n    script</BlogLink> *(also known as Roman script)* is based on the classical Latin alphabet used by the Romans and is one of the oldest and most widely adopted writing systems in the world. It's the foundation for many modern languages including English, French, German, Italian, Portuguese and many more.\r\nIt has evolved to include diacritical marks (e.g., accents like √©, umlauts like √º) to adapt to various languages.\r\n\r\n<BlogLink href=\"https://en.wikipedia.org/wiki/Cyrillic_script\">Cyrillic\r\n    script</BlogLink> is also a very common script and is used for Russian, Bulgarian, Serbian, Ukrainian, Belarusian, Mongolian and more.\r\nCreated in the 9th century by Saints Cyril and Methodius to write Old Church Slavonic, the Cyrillic script has since been adapted for numerous languages and it often includes additional letters and modifications to suit the phonetics of specific languages.\r\n\r\n\r\nUnicode supports current and historic scripts, including extinct languages and as of Unicode v16.0 released in September 2024, the standard includes 168 scripts *(that's a lot - we only very briefly talked about two!)*.\r\n\r\n### Symbols\r\nSymbols are characters that are not part of a writing-system (a script), but serve other purposes such as emojis, mathmatical operators or geometric shapes for instance. They may be:\r\n* Mathematical symbols `+`, `-`, `‚àë`, `‚à´`\r\n* Currency symbols `$`, `‚Ç¨`, `¬•`, `‚Çπ`\r\n* Emojis `üòä`, `üòÇ`, `ü•≤`, `üçé`, `üçï`, `üêç`, `üêò`\r\n\r\n\r\nYou can easily test whether a Unicode character is a symbol or part of a script in Python:\r\n```python\r\ndef is_symbol(char):\r\n    \"\"\"\r\n    So: Other symbols (e.g., ‚ôî, ‚ô´)\r\n    Sc: Currency symbols (e.g., $)\r\n    Sm: Mathematical symbols (e.g., +, √ó)\r\n    Sk: Modifier symbols (e.g., ^, ~)\r\n    \"\"\"\r\n    category = unicodedata.category(char)\r\n    return category.startswith(\"S\")  # an 'S'-prefix indicates symbol categories\r\n```\r\n\r\n### The Intersection of Scripts & Symbols\r\nScripts and symbols are not rigid, mutually exclusive categories in Unicode. Instead, scripts often contain symbols, and the distinction between them can be fluid, depending on how they are used and categorized in Unicode.\r\nWhile a script is usually thought of as the collection of alphabetic characters or logograms, many scripts also combes bundled with symbols, such as:\r\n* Latin Script: `@`, `$`, `%`, `&`\r\n\r\nThis will become a bit more clear, once we start talking about code blocks and code points.\r\n\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer Three: Code Points and Code Blocks\r\n### Code Blocks & Code Points\r\nUnicode is a semantic system working in so-called code-points, it doesn't handle appearances - for that we have fonts. Rendered code points are referred to as glyphs and may vary across fonts.\r\nUnicode uses two other concepts that are functionally significant of code blocks and code points. Unicode assigns each character a unique numerical (hexadecimal) value preprended with `U+`, e.g. `U+2638`.\r\n\r\nCodepoints range from `U+0000` to `U+10FFFF`. The set of all code points are divided into contiguous ranges for different languages and categories.\r\n\r\n### Characters, not glyphs!\r\nUnicode standardizes characters and not glyphs which is the visual representation of a character. Different fonts can render the same Unicode code point (character) with different representations and typographic styles - these different representations of codepoints/characters are called glyhphs.\r\n\r\n<ZoomableImage src=\"/graphics/blog/10-minutes-of-unicode/glyph-representation-of-characters.png\" maxWidth=\"75%\"/>\r\n\r\n\r\nUnicode is therefore not a text-encoding scheme; it‚Äôs a map of code points (numerical values) that point to characters and it's used by encodings like UTF-8, UTF-16, and UTF-32.\r\nUnicode describes neither how those characters are stored in memory or transmitted as bytes - for that we use encoding like UTF-8, UTF-16 or UTF-32 (UTF: Unicode Transformation Format).\r\n\r\nSome scripts, like Armenian, represent only one language, while others, like Latin, are used by multiple languages, including English, French, and Vietnamese.\r\n\r\n\r\nBasic Latin script (see image) spans code points U+0000 to U+007F and includes the English alphabet. If we convert the codepoints from hex to decimal, you'll notice the range is 0-127 which is the exact same codepoints used in standard ASCII for the same characters - this is because Unicode is backwards compatible with ASCII.\r\nThis means that any text encoded in ASCII can be safely interpreted by any system that uses Unicode.\r\n\r\nIf you take the string \"Hello\", it can be represented as:\r\n\r\n* In ASCII: `H = 72`, `e = 101`, `l = 108`, `o = 111` (all in the 0‚Äì127 range).\r\n* In Unicode: The code points for \"Hello\" in Unicode are `U+0048` (H), `U+0065` (e), `U+006C` (l), `U+006C` (l), `U+006F` (o). These are identical to ASCII code points.\r\n\r\n<ZoomableImage src=\"/graphics/blog/10-minutes-of-unicode/unicode-basic-latin.png\" maxWidth=\"60%\"></ZoomableImage>\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer Four: Unicode Planes\r\n(Note: You haven't written anything yet specifically about Planes ‚Äî BMP, SMP, etc. I can generate a fitting paragraph here if you want.)\r\n\r\nPlaceholder: To Be Written\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer Five: Encodings ‚Äî UTF-8, UTF-16, UTF-32\r\nUTF-8 is a variable-length encoding that represents Unicode code points in 1 to 4 bytes. It‚Äôs backward-compatible with ASCII, meaning ASCII text is also valid UTF-8 text.\r\n\r\nExample:\r\n- The letter \"A\" (code point U+0041) in UTF-8 is encoded as a single byte (0x41).\r\n\r\nUTF-8 is widely used on the web and is often the default encoding in systems, programming languages, and applications.\r\n\r\n### Notes on Encoding Variants\r\n- UTF-16 uses 2 or 4 bytes per character, which can be beneficial for languages with many non-ASCII characters but has disadvantages in storage efficiency.\r\n- UTF-32 uses 4 bytes per character uniformly, offering simple processing but with significant storage costs.\r\n\r\nIn most applications, UTF-8 is the best choice due to its compatibility and efficiency but they all have their place, which I'll get to later.\r\n\r\n### Why the Fuss About Unicode?\r\nWhy not just stick with ASCII? ASCII only covers 128 characters ‚Äî barely enough for basic English. Unicode, however, aims to cover all text systems in a single, universal standard, eliminating the confusion and compatibility issues of past encoding schemes.\r\n\r\nIn the early 90s, the internet accelerated Unicode‚Äôs adoption as more text needed to be exchanged globally. Unicode incorporates characters from legacy encodings (e.g., Windows-1252) to ensure backward compatibility.\r\n\r\n### Common Unicode Myths\r\n1. **\"Unicode is 16-bit and has 65,536 characters.\"**\r\nIncorrect. Unicode is not limited to 16 bits. While early Unicode proposals suggested a 16-bit system, Unicode today spans over 1 million code points (0 to 0x10FFFF).\r\n\r\n2. **\"Unicode is an encoding.\"**\r\nAlso incorrect. Unicode is a standard, not an encoding. UTF-8, UTF-16, and UTF-32 are encoding formats that define how to represent Unicode code points in bytes.\r\n\r\n### Practical Advice for Working with Unicode\r\nHandling Unicode effectively requires knowing:\r\n- The level of representation you‚Äôre dealing with ‚Äî code points, bytes, or glyphs.\r\n- Encoding consistency ‚Äî never assume encoding, especially for data from unknown sources.\r\n\r\n*Modern systems default to UTF-8, but misinterpretations can still occur, often with ISO-8859-1 (latin1) or cp1252 on legacy systems.*\r\n\r\n### Exploring Unicode in Python\r\nHere's a Python example to illustrate basic Unicode encoding and decoding.\r\n<ZoomableImage src={\"/graphics/blog/10-minutes-of-unicode/string_to_x.png\"} sx={{maxWidth: \"70vw\"}}></ZoomableImage>\r\n\r\n\r\n```shell{outputLines: 2-10,12}{promptUser: python}\r\n print(chr(0x1F64F)) # (Unicode character represented by code point U+1F64F)\r\n print(chr(0x1F64F)) # (Unicode character represented by code point U+1F64F)\r\n üôè\r\n # Also, chr() creates characters from code points.\r\n # We could do chr(0xFF) or chr(33)\r\n print(hex(ord(\"üôè\"))) # -> 0x1f64f\r\n # ord() is the reverse operation - giving us the base10 codepoint for a specific character\r\n [ord(i) for i in \"hello world\"] # -> [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]\r\n\r\n # We can represent characters in many analogue ways\r\n\"\"\"\r\nEscape Sequence     Meaning                                           How To Express \"a\"\r\n\"\\ooo\"              Character with octal value ooo                    \"\\141\"\r\n\"\\xhh\"              Character with hex value hh                       \"\\x61\"\r\n\"\\N{name}\"          Character named name in the Unicode database      \"\\N{LATIN SMALL LETTER A}\"\r\n\"\\uxxxx\"            Character with 16-bit (2-byte) hex value xxxx     \"\\u0061\"\r\n\"\\Uxxxxxxxx\"        Character with 32-bit (4-byte) hex value xxxxxxxx \"\\U00000061\"\r\n\"\"\"\r\nprint(\r\n     \"a\" ==\r\n     \"\\x61\" ==\r\n     \"\\N{LATIN SMALL LETTER A}\" ==\r\n     \"\\u0061\" ==\r\n     \"\\U00000061\"\r\n     ) # Prints `True` as they are all identical\r\n # bin, hex and oct are only representations, not a fundamental change in the input.\r\n     >>> import unicodedata\r\n >>> unicodedata.name(\"‚Ç¨\")\r\n 'EURO SIGN'\r\n >>> unicodedata.lookup(\"EURO SIGN\")\r\n '‚Ç¨'\r\n str(b\"\\xc2\\xbc cup of flour\", \"utf-8\") # -> '¬º cup of flour'\r\n str(0xc0ffee) # -> '12648430'\r\n\r\n text = \"Hello, Unicode! üåç\"\r\n utf8_encoded = text.encode('utf-8')\r\n print(f\"UTF-8 Encoded: {utf8_encoded}\")\r\n utf8_decoded = utf8_encoded.decode('utf-8')\r\n print(f\"Decoded Text: {utf8_decoded}\")\r\n\r\n emoji = \"ü§®\"  # U+1F928\r\n print(f\"Emoji bytes in UTF-8: {emoji.encode('utf-8')}\")\r\n```\r\n\r\nEncoding text with UTF-8 converts characters into byte representations, which is crucial for I/O operations.\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer 6: Unicode Properties\r\nLayer Six: Unicode Properties\r\n(Note: You haven't specifically explained General Categories, Bidi properties, Numeric values, Combining classes yet.)\r\n\r\nPlaceholder: To Be Written\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer 7: Unicode Normalization\r\nLayer Seven: Unicode Normalization\r\n(Note: You haven't yet explained NFC, NFD, NFKC, NFKD normalization.)\r\n\r\nPlaceholder: To Be Written\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer 8: Unicode Security Issues\r\nLayer Eight: Unicode Security Issues\r\n(Note: You haven't yet covered homoglyph attacks, Trojan Source attacks, zero-width attacks.)\r\n\r\nPlaceholder: To Be Written\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer 9: Unicode in Systems and Protocols\r\n### Why There‚Äôs No Such Thing as \"Plain Text\" (continued)\r\n\r\n(Your examples fit here too because you explain encoding in internet protocols, HTTP headers, etc.)\r\n\r\n- Over the internet, it's specified in HTTP headers...\r\n- In HTML/XML, it's in `<meta charset=\"UTF-8\">`...\r\n<Divider sx={{my: 4, mx: 2}}></Divider>\r\n\r\n## Layer 10: Advanced Unicode Algorithms\r\nLayer Ten: Advanced Unicode Algorithms\r\n(Note: You haven't yet covered Unicode Text Segmentation (TR29), Collation (sorting), Bidirectional Algorithm.)\r\n\r\nPlaceholder: To Be Written\r\n\r\n<Typography variant=\"h3\">Further Reading</Typography>\r\nhttps://peps.python.org/pep-0263/ PEP 263 ‚Äì Defining Python Source Code Encodings\r\n\r\n\r\n| Encoding     | Byte Length  | Supports            | Use Cases                              |\r\n|--------------|--------------|---------------------|----------------------------------------|\r\n| ASCII        | 1 byte       | Basic English       | Legacy systems, programming, protocols |\r\n| ISO-8859-1   | 1 byte       | Western Europe      | Web pages, legacy databases, email     |\r\n| UTF-16       | 2 or 4 bytes | All Unicode         | Java, Windows, some databases          |\r\n| UTF-32       | 4 bytes      | All Unicode         | Some internal systems, databases       |\r\n| Windows-1252 | 1 byte       | Western Europe      | Legacy Windows apps, web pages         |\r\n| Shift-JIS    | 1 or 2 bytes | Japanese            | Japanese text encoding                 |\r\n| GB2312       | 2 bytes      | Simplified Chinese  | Chinese mainland web pages, files      |\r\n| Big5         | 2 bytes      | Traditional Chinese | Taiwan, Hong Kong legacy systems       |\r\n| EBCDIC       | 1 byte       | IBM Mainframes      | IBM mainframe systems                  |\r\n","frontmatter":{"slug":"blog/ten-minutes-of-unicode","date":"2025-04-28","title":"From ASCII to Unicode","description":"Dive deep into the layers of Unicode, the universal text encoding standard ‚Äî from its origins in ASCII to scripts, code points, normalization, and security implications.","categories":["Computer Science","Systems Programming"],"tags":["Unicode","Character Encoding","Text Encoding","Python","Security"],"featured":true,"series":"Understanding Systems Deeply"}}}]}}}